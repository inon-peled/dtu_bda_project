{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Data Analysis Project: U.S. Births\n",
    "### DTU, 2018\n",
    "### Team Members: Inon Peled, Niklas Christopher Petersen, Mads Paulsen. \n",
    "\n",
    "# Introduction\n",
    "TODO: The introduction is inviting, presents an overview of the notebook. Information is relevant and presented in a logical order. The notebook presents a clear cohesive data analysis story, which is enjoyable to read. Structure and organization. What is the data and the analysis problem.\n",
    "\n",
    "The model can be something already used in the course or something else, but donâ€™t try to do\n",
    "too complex things. Ther are separate project courses work for more complex models. So mention similarity to some homework models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Commonly Used Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import pandas as pd\n",
    "import pystan\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 16, 6\n",
    "COMMON_SEED = 1234\n",
    "\n",
    "def check_convergence(fit, also_print=False):\n",
    "    report = print if also_print else lambda x: None\n",
    "    \n",
    "    def all_rhat_small_enough(fit):\n",
    "        return all(dict(fit.summary())['summary'][:, -1] < 1.1)\n",
    "    \n",
    "    def max_treedepth_exceeded(fit, max_depth = 10):\n",
    "        \"\"\"Check transitions that ended prematurely due to maximum tree depth limit\"\"\"\n",
    "        sampler_params = fit.get_sampler_params(inc_warmup=False)\n",
    "        depths = [x for y in sampler_params for x in y['treedepth__']]\n",
    "        n = sum(1 for x in depths if x == max_depth)\n",
    "        if n > 0:\n",
    "            report('Run again with max_depth set to a larger value to avoid saturation')        \n",
    "        N = len(depths)\n",
    "        report(('{} of {} iterations saturated the maximum tree depth of {}'\n",
    "               + ' ({}%)').format(n, N, max_depth, 100 * n / N))\n",
    "        return float(n) / N\n",
    "    \n",
    "    def e_bfmi_all_low_enough(fit):\n",
    "        \"\"\"\n",
    "        Checks the energy Bayesian fraction of missing information (E-BFMI).\n",
    "        E-BFMI below 0.2 indicates you may need to reparameterize your model\n",
    "        \"\"\"\n",
    "        sampler_params = fit.get_sampler_params(inc_warmup=False)\n",
    "        for chain_num, s in enumerate(sampler_params):\n",
    "            energies = s['energy__']\n",
    "            numer = sum((energies[i] - energies[i - 1])**2 for i in range(1, len(energies))) / len(energies)\n",
    "            denom = np.var(energies)\n",
    "            if numer / denom < 0.2:\n",
    "                report('Chain {}: E-BFMI = {}'.format(chain_num, numer / denom))\n",
    "                report('E-BFMI below 0.2 indicates you may need to reparameterize your model')\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def fraction_of_transitions_which_ended_with_divergence(fit):\n",
    "        \"\"\"Check transitions that ended with a divergence\"\"\"\n",
    "        sampler_params = fit.get_sampler_params(inc_warmup=False)\n",
    "        divergent = [x for y in sampler_params for x in y['divergent__']]\n",
    "        n = sum(divergent)\n",
    "        N = len(divergent)\n",
    "        report('{} of {} iterations ended with a divergence ({}%)'\n",
    "              .format(n, N, 100 * n / N))\n",
    "        if n > 0:\n",
    "            report('Try running with larger adapt_delta to remove the divergences')\n",
    "        return n / N\n",
    "    \n",
    "    assert all_rhat_small_enough(fit)\n",
    "    assert max_treedepth_exceeded(fit) < 0.02\n",
    "    assert e_bfmi_all_low_enough(fit)\n",
    "    assert fraction_of_transitions_which_ended_with_divergence(fit) == 0\n",
    "    report(\"##### All convergence checks passed successfully. #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "Before fitting prediction models, let us look into the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>births</th>\n",
       "      <th>weekday_name</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-12-27</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>8656</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>361</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-28</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>7724</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>362</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-29</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>12811</td>\n",
       "      <td>Monday</td>\n",
       "      <td>363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-30</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>13634</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>11990</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            year  month  day  day_of_week  births weekday_name  day_of_year  \\\n",
       "2014-12-27  2014     12   27            6    8656     Saturday          361   \n",
       "2014-12-28  2014     12   28            7    7724       Sunday          362   \n",
       "2014-12-29  2014     12   29            1   12811       Monday          363   \n",
       "2014-12-30  2014     12   30            2   13634      Tuesday          364   \n",
       "2014-12-31  2014     12   31            3   11990    Wednesday          365   \n",
       "\n",
       "            week_of_year  \n",
       "2014-12-27            52  \n",
       "2014-12-28            52  \n",
       "2014-12-29             1  \n",
       "2014-12-30             1  \n",
       "2014-12-31             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births_2000s_df = pd.read_csv('US_births_2000-2014_SSA.csv')\n",
    "births_2000s_df_with_index = births_2000s_df\\\n",
    "    .rename(columns={'date_of_month': 'day'})\\\n",
    "    .set_index(pd.to_datetime(\n",
    "        births_2000s_df.rename(columns={'date_of_month': 'day'})\n",
    "        [['year', 'month', 'day']]))\\\n",
    "    .assign(weekday_name=lambda df: df.index.weekday_name)\\\n",
    "    .assign(day_of_year=lambda df: df.index.dayofyear)\\\n",
    "    .assign(week_of_year=lambda df: df.index.weekofyear)\n",
    "births_2000s_df_with_index.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assert that no data imputation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_days_in_year(year):\n",
    "    return (datetime(year, 12, 31) - datetime(year, 1, 1)).days + 1\n",
    "\n",
    "\n",
    "def missing_days(df):\n",
    "    return births_2000s_df_with_index.groupby('year').births.describe()\\\n",
    "        .assign(expected_days=lambda df: list(map(num_days_in_year, df.index)))\\\n",
    "        .assign(missing=lambda df: df.expected_days - df['count'])\n",
    "\n",
    "        \n",
    "assert not any(missing_days(births_2000s_df_with_index).missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next show how births distribute in each year, month, and day of week. We see that whereas the data follows a somewhat steady mean and std. dev. over years and over months, there are is much more variability between days of week:\n",
    "1. There are numerous outliers in each day of week.\n",
    "2. There are significantly _less_ births during the weekend (Saturday-Sunday) than during the work week (Monday-Friday).\n",
    "3. Weekend days have lower variance than work days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_mean_and_std_by_time_dimension(time_dimension):\n",
    "#     births_2000s_df_with_index.groupby(time_dimension).births.describe()[['mean', 'std']].plot.bar(fontsize=14)\n",
    "\n",
    "# list(map(show_mean_and_std_by_time_dimension, ['weekday_name', 'year', 'month']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots(df, time_dimension):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    x = sorted(df[time_dimension].unique())\n",
    "    ax.boxplot([list(df[df[time_dimension] == t].births) for t in x])\n",
    "    ax.set_xticklabels(x)\n",
    "    ax.set_xlabel(df[time_dimension].name)\n",
    "\n",
    "    \n",
    "for time_dimension in ('year', 'month', 'weekday_name'):\n",
    "    boxplots(births_2000s_df_with_index, time_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also take a look at the distribution of all data points, regardless of time. The distribution is markedly bimodal: one mode for weekend, the other for rest of week. In fact, the weekend distribution is bimodal too, because of differences between Saturday and Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation(df):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    births_weekend = df.births[df.day_of_week >= 6]\n",
    "    births_work_days = df.births[df.day_of_week < 6]\n",
    "    \n",
    "    ax.hist(births_weekend, bins='auto', facecolor='blue', edgecolor='white', \n",
    "            normed=True, label=\"Histogram, Weekend\", alpha=0.3)\n",
    "    ax.hist(births_work_days, bins='auto', facecolor='red', edgecolor='white', \n",
    "            normed=True, label=\"Histogram, Work Days\", alpha=0.3)\n",
    "\n",
    "    x_kde_weekend = np.linspace(births_weekend.min(), births_weekend.max(), 100)\n",
    "    ax.plot(x_kde_weekend, gaussian_kde(births_weekend)(x_kde_weekend),\n",
    "            label=\"KDE, Weekend\", color='blue')\n",
    "\n",
    "    x_kde_work_days = np.linspace(births_work_days.min(), births_work_days.max(), 100)\n",
    "    ax.plot(x_kde_work_days, gaussian_kde(births_work_days)(x_kde_work_days), label=\"KDE, Work Days\", color='red')\n",
    "    \n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.legend()\n",
    "    \n",
    "density_estimation(births_2000s_df_with_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we turn to look at Gaussian Kernel Density Estimation (KDE) for each weekday separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation_by_weekday(df):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    for d in df.weekday_name.unique():\n",
    "        daily_births = df.births[df.weekday_name == d]\n",
    "        x_kde = np.linspace(daily_births.min(), daily_births.max(), 100)\n",
    "        ax.plot(x_kde, gaussian_kde(daily_births)(x_kde), label=\"KDE, %s\" % d, lw=3)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_xlabel(\"Births\")\n",
    "    ax.legend(fontsize=14)\n",
    "    \n",
    "density_estimation_by_weekday(births_2000s_df_with_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality\n",
    "The next few plots are meant to emphasize any obvious seasonality in the time series of births.\n",
    "\n",
    "First, we see that each year displays are generally similar trend: steady increase in first few months, followed by a peak, a drop, and another peak at end of year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_births(df):\n",
    "    d = df.groupby(('year', 'month')).births.sum().reset_index()\n",
    "    f = pd.to_datetime(d.assign(day=1).drop('births', axis=1))\n",
    "    ax = d.set_index(f).drop(['year', 'month'], axis=1)\\\n",
    "        .births.rename(\"Monthly Births\")\\\n",
    "        .plot(color=\"black\", marker='o', markersize=4)\n",
    "    ax.legend()\n",
    "    ax.grid(which='both')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plot_monthly_births(births_2000s_df_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_some_years(years):\n",
    "    matplotlib.rc('font', **{'size': 18})\n",
    "    fix, axes = plt.subplots(len(years), 1, sharex=True)\n",
    "    for i, year in enumerate(years):\n",
    "        axes[i].plot(\n",
    "            births_2000s_df_with_index.set_index('day_of_year')[lambda r: r.year == year].births, \n",
    "            label=year, color='blue')\n",
    "        axes[i].legend(prop={'weight':'bold'}, loc='center', framealpha=1)\n",
    "        axes[i].get_yaxis().set_visible(False)\n",
    "    axes[-1].set_xlabel(\"Day of Year\")\n",
    "    plt.tight_layout()\n",
    "        \n",
    "compare_some_years(range(2000, 2015, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that people are biased against giving births in some specific dates: holidays, 13th of each month, and 29th Feb. on leap years. However, in this work, we will not involve any contextual information, but rather use only the births data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Models and Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following describes each Bayesian Inference model we test, along with justifications for choosing such a model, and a definition of the **prior distributions** which the model uses. All models work on standardized data, i.e. for each data point in a train set, we remove the train set mean and then divide by the train set std. dev.\n",
    "\n",
    "## Prior assumptions for all models\n",
    "We take figures from year 2000 for establishing (strongly) informative priors about daily number of births, which in turn yields quicker convergence. The train set then starts at year 2001.\n",
    "\n",
    "### Models Disregarding the Time Series\n",
    "The plot of KDEs shows that for each weekday, the data distribution resembles a Gaussian. We should thus try several models:\n",
    "1. Searate Model: for each weekday $d=1..7$, assign parameters $\\mu_d, \\sigma_d>0$, each with prior $\\mathcal{N}(0, 10)$, and let $y_n \\sim \\mathcal{N} \\left( \\mu_{x_n}, \\sigma_{x_n}^2 \\right)$, where $x_n$ is day-of-week.\n",
    "2. Hierarchical Model: similar to Separate Model, except that all $\\mu_d$ and all $\\sigma_d$ are each drawn from a shared distribution.\n",
    "3. Hierarchicial Model for work days, Separate Models for weekend days.\n",
    "\n",
    "### Models Using the Time Series\n",
    "1. For each day $t$, let\n",
    "$$\n",
    "x_t = \\left( \\text{year}_t, \\text{month}_t, \\text{day-of-month}_t, \\text{day-of-week}_t \\right)\n",
    "$$\n",
    "We model the number of births $y_t$ as following a prior Poisson distribution, with rate determined by data in days up to $t-1$, namely\n",
    "$$\n",
    "y_t \\sim Poisson \\left( exp\\left(z_t\\right) \\right)\n",
    "$$\n",
    "where the exponent ensures that the Poisson rate is positive, and\n",
    "$$\n",
    "z_t \\sim \\mathcal{N}\\left(\\mu_t, \\sigma_t^2\\right)\n",
    "$$\n",
    "so that \n",
    "$$\n",
    "\\mu_t = \\beta_\\mu z_{t-1} + \\theta_\\mu^T x_t \\\\\n",
    "\\sigma_t = \\beta_\\sigma z_{t-1} + \\theta_\\sigma^T x_t\n",
    "$$\n",
    "The parameters to be learnt are $\\beta_\\mu, \\beta_\\sigma, \\theta_mu, \\theta_\\sigma$, each with prior $\\mathcal{N}(0, 10)$. In accordance with our data analysis, this model is heteroskedastic, i.e. allows a different variance $\\sigma_t$ for each $t$.\n",
    "\n",
    "2. Same as the former option, except that\n",
    "$$\n",
    "y_t \\sim NB \\left( r, \\frac{exp(z_t)}{exp(z_t) + r} \\right)\n",
    "$$\n",
    "Where NB is the Negative Binomial distribution, and $r$ is another parameter to be learnt. The Negative Binomial distribution allows for variance different than mean, and so is more robust than the Poisson distribution, where the mean and variance are the same.\n",
    "\n",
    "TODO: Required onvergence diagnostic results (Rhat, neff, divergences) shown and maning of the results is discussed.\n",
    "TODO: PGM drawings from e.g. ShinyStan?\n",
    "TODO: A table summarizing the Stan parameters (chains, algorithm, etc.) we use in running each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for Weekdays\n",
    "We begin by obtaining prior mean and standard deviation for each weekday in year 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekday_priors_by_year_2000():\n",
    "    df_2000 = births_2000s_df_with_index[lambda df: df.year == 2000]\n",
    "    return df_2000.groupby('day_of_week').agg(['mean', 'std']).births\n",
    "\n",
    "weekday_priors_by_year_2000()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline: Historical Average for Each Day Separately\n",
    "For HA, there is no point in running a STAN model: our predictions will anyway be the hisotrical mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = births_2000s_df_with_index.day_of_week.values\n",
    "y = births_2000s_df_with_index.births.values\n",
    "N = len(y)\n",
    "print(x, y, N, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_for_sliding_window(test_date):\n",
    "    num_data_points_before_first_test_date = datetime(2014, 1, 1) - datetime(2001, 1, 1)\n",
    "    df = births_2000s_df_with_index[['day_of_week', 'births']]\\\n",
    "        [lambda df: df.index <= test_date + timedelta(days=6)]\\\n",
    "        [lambda df: df.index >= test_date - num_data_points_before_first_test_date]\n",
    "    return {\n",
    "        'x_train': df.day_of_week[:-7].values,\n",
    "        'y_train': df.births[:-7].values,\n",
    "        'x_test': df.day_of_week[-7:].values,\n",
    "        'y_test': df.births[-7:].values\n",
    "    }\n",
    "        \n",
    "get_train_and_test_for_sliding_window(datetime(2014, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_average(x_train, y_train):\n",
    "    ha = np.empty(7)\n",
    "    for i in range(7):\n",
    "        ha[i] = y_train[x_train - 1 == i].mean()\n",
    "    return ha\n",
    "\n",
    "historical_average(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ha_sliding_windows(num_windows):\n",
    "    prediction_errors = []\n",
    "    for i in range(num_windows):\n",
    "        data = get_train_and_test_for_sliding_window(datetime(2014, 1, 1) + timedelta(days=i * 7))\n",
    "#         print(data)\n",
    "        y_pred = historical_average(data['x_train'], data['y_train'])[data['x_test'] - 1].round()\n",
    "        prediction_errors.append(y_pred - data['y_test'])\n",
    "#         print(y_pred)\n",
    "#         print(data['y_test'])\n",
    "    return np.concatenate(prediction_errors)\n",
    "\n",
    "def rmse(prediction_errors):\n",
    "    return (prediction_errors ** 2).mean() ** 0.5\n",
    "\n",
    "rmse(fit_ha_sliding_windows(num_windows=51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Model\n",
    "In the separate model, each weekday $j$ is given an independent prior $\\text{Cauchy}(\\mu_j, \\sigma_j)$, where $\\mu_j$ equal to the sample mean and $\\sigma_j$ equal to the sample standard deviation for weekday $j$. We choose the Cauchy distribution because it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_separate_weekdays = pystan.StanModel(model_code='''\n",
    "data {\n",
    "    int<lower=0> T; // Number of predictions.\n",
    "    int<lower=0> N; // Number of data points.\n",
    "    vector[N] y_train; // Train data points.\n",
    "    vector[T] y_test; // Test data points, for log-likelihood.\n",
    "    int<lower=1,upper=7> x_train[N]; // Weekday indicator for each observed data point.\n",
    "    int<lower=1,upper=7> x_test[T]; // Weekday indicator for each prediction.\n",
    "    vector[7] prior_means;\n",
    "    vector[7] prior_stds;\n",
    "    real<lower=0> V_mu; // Std. dev. for priors on mu.\n",
    "    real<lower=0> V_sigma; // Std. dev. for priors on sigma.\n",
    "}\n",
    "parameters {\n",
    "    real mu[7];    // Separate means.\n",
    "    real<lower=0> sigma[7]; // Separate std. devs.\n",
    "}\n",
    "model {\n",
    "    mu ~ normal(prior_means, V_mu * prior_stds);\n",
    "    sigma ~ normal(prior_stds, V_sigma * prior_stds);\n",
    "    y_train ~ normal(mu[x_train], sigma[x_train]);\n",
    "}\n",
    "generated quantities {\n",
    "    real loglik;\n",
    "    vector[T] y_pred;\n",
    "    loglik = 0;\n",
    "    for (t in 1:T) {\n",
    "        y_pred[t] = normal_rng(mu[x_test[t]], sigma[x_test[t]]);\n",
    "        loglik += normal_lpdf(y_test[t] | mu[x_test[t]], sigma[x_test[t]]);\n",
    "    }\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_train_and_test_for_sliding_window(datetime(2014, 1, 1) + timedelta(days=7))\n",
    "fit = model_separate_weekdays.sampling(seed=COMMON_SEED, data=dict(\n",
    "    prior_means=weekday_priors_by_year_2000()['mean'].values,\n",
    "    prior_stds=weekday_priors_by_year_2000()['std'].values,\n",
    "    V_mu=4,\n",
    "    V_sigma=0.5,\n",
    "    x_train=data['x_train'],\n",
    "    x_test=data['x_test'],\n",
    "    y_train=data['y_train'],\n",
    "    y_test=data['y_test'],\n",
    "    T=7,\n",
    "    N=len(data['x_train'])), iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_convergence(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_separate_sliding_windows(num_windows):\n",
    "    prediction_errors = []\n",
    "    for i in range(num_windows):\n",
    "        data = get_train_and_test_for_sliding_window(datetime(2014, 1, 1) + timedelta(days=i * 7))\n",
    "        fit = model_separate_weekdays.sampling(seed=COMMON_SEED, data=dict(\n",
    "            prior_means=weekday_priors_by_year_2000()['mean'].values,\n",
    "            prior_stds=weekday_priors_by_year_2000()['std'].values,\n",
    "            V_mu=4,\n",
    "            V_sigma=0.5,            \n",
    "            x_train=data['x_train'],\n",
    "            x_test=data['x_test'],\n",
    "            y_train=data['y_train'],\n",
    "            y_test=data['y_test'],\n",
    "            T=7,\n",
    "            N=len(data['x_train'])), iter=500)\n",
    "        y_pred = fit.extract()['y_pred'].mean()\n",
    "        prediction_errors.append(fit.extract()['y_pred'].mean(axis=0) - data['y_test'])\n",
    "    return np.concatenate(prediction_errors)\n",
    "\n",
    "rmse(fit_separate_sliding_windows(num_windows=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stan_model_on_sliding_windows(persist_path, stan_model, num_windows):\n",
    "    if not os.path.exists(persist_path):\n",
    "        os.mkdir(persist_path)\n",
    "    all_prediction_errors = []\n",
    "    for i in range(num_windows):\n",
    "        data = get_train_and_test_for_sliding_window(datetime(2014, 1, 1) + timedelta(days=i * 7))\n",
    "        fit = stan_model.sampling(seed=COMMON_SEED, data=dict(\n",
    "            prior_means=weekday_priors_by_year_2000()['mean'].values,\n",
    "            prior_stds=weekday_priors_by_year_2000()['std'].values,\n",
    "            V_mu=4,\n",
    "            V_sigma=0.5,            \n",
    "            x_train=data['x_train'],\n",
    "            x_test=data['x_test'],\n",
    "            y_train=data['y_train'],\n",
    "            y_test=data['y_test'],\n",
    "            T=7,\n",
    "            N=len(data['x_train'])), iter=500)\n",
    "        y_pred = fit.extract()['y_pred'].mean()\n",
    "        pred_err = fit.extract()['y_pred'].mean(axis=0) - data['y_test']\n",
    "        all_prediction_errors.append(pred_err)\n",
    "        with open(os.path.join(persist_path, 'fit%d_summary.pkl' % i), 'wb') as f:\n",
    "            pickle.dump(fit.summary(), f)\n",
    "        with open(os.path.join(persist_path, 'fit%d_extract.pkl' % i), 'wb') as f:\n",
    "            pickle.dump(fit.extract(), f)         \n",
    "        with open(os.path.join(persist_path, 'fit%d_pred_err.pkl' % i), 'wb') as f:\n",
    "            pickle.dump(pred_err, f)\n",
    "    return rmse(np.concatenate(all_prediction_errors))\n",
    "\n",
    "fit_stan_model_on_sliding_windows('fit_separate', model_separate_weekdays, 51)\n",
    "# pickle.load(open('./fit_separate/fit0_extract.pkl', 'rb'))['mu'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stan Model: 2 Hierarchical Models, Poisson\n",
    "We fit a distribution for each weekday, so that parameters for weekend days (Sat., Sun.) are derived from one distribution, and parameters for the other days (Mon.-Fri.) are derived from another distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_weekend</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>12481.507692</td>\n",
       "      <td>909.221295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>8532.132075</td>\n",
       "      <td>614.062616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean         std\n",
       "is_weekend                          \n",
       "False       12481.507692  909.221295\n",
       "True         8532.132075  614.062616"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def priors_for_weekend_vs_workday_by_year_2000():\n",
    "    df_2000 = births_2000s_df_with_index[lambda df: df.year == 2000]\n",
    "    return df_2000\\\n",
    "        .assign(is_weekend=lambda df: df.day_of_week.isin([6, 7]))\\\n",
    "        .groupby('is_weekend')\\\n",
    "        .agg(['mean', 'std'])\\\n",
    "        .births\n",
    "\n",
    "priors_for_weekend_vs_workday_by_year_2000()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hier_poisson = pystan.StanModel(model_code='''\n",
    "data {\n",
    "    int<lower=0> T; // Number of predictions.\n",
    "    int<lower=0> N; // Number of data points.\n",
    "    vector[N] y_train; // Train data points.\n",
    "    vector[T] y_test; // Test data points, for log-likelihood.\n",
    "    int<lower=1,upper=7> x_train[N]; // Weekday indicator for each observed data point.\n",
    "    int<lower=1,upper=7> x_test[T]; // Weekday indicator for each prediction.\n",
    "    real<lower=0> prior_mean_weekend;\n",
    "    real<lower=0> prior_mean_workday;\n",
    "    real<lower=0> prior_std_weekend;\n",
    "    real<lower=0> prior_std_workday;\n",
    "    real<lower=0> V_lambda; // Std. dev. for priors on lambda.\n",
    "    real<lower=0> V_sigma; // Std. dev. for priors on sigma.\n",
    "}\n",
    "parameters {\n",
    "    real<lower=0> lambda[7];\n",
    "    real<lower=0> lambda_weekend;\n",
    "    real<lower=0> lambda_workday;\n",
    "    real<lower=0> sigma_weekend;\n",
    "    real<lower=0> sigma_workday;\n",
    "}\n",
    "model {\n",
    "    lambda_weekend ~ normal(prior_mean_weekend, V_lambda * prior_std_weekend)\n",
    "    lambda_workday ~ normal(prior_mean_workday, V_lambda * prior_std_workday)\n",
    "    sigma_weekend ~ normal(prior_std_weekend, V_sigma * prior_std_weekend);\n",
    "    sigma_workday ~ normal(prior_std_workday, V_sigma * prior_std_workday);\n",
    "    lambda[6] ~ normal(lambda_weekend, sigma_weekend);\n",
    "    lambda[7] ~ normal(lambda_weekend, sigma_weekend);\n",
    "    for (i in 1:5) {\n",
    "        lambda[i] ~ normal(lambda_workday, sigma_workday);\n",
    "    }\n",
    "    y ~ poisson(lambda[x_train]);\n",
    "}\n",
    "generated quantities {\n",
    "    real loglik;\n",
    "    vector[T] y_pred;\n",
    "    loglik = 0;\n",
    "    for (t in 1:T) {\n",
    "        y_pred[x_test[i]] = poisson_rng(lambda[x_test[i]]);\n",
    "        loglik += poisson_lpmf(y_test[t] | lambda[x_test[t]]);\n",
    "    }\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_train_and_test_for_sliding_window(datetime(2014, 1, 1) + timedelta(days=7))\n",
    "fit = model_hier_poisson.sampling(seed=COMMON_SEED, data=dict(\n",
    "    x_train=data['x_train'],\n",
    "    x_test=data['x_test'],\n",
    "    y=data['y_train'],\n",
    "    T=7,\n",
    "    N=len(data['x_train'])), iter=500)\n",
    "# check_convergence(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_convergence(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.extract()['lambda'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.extract()['y_pred'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stan_model_sliding_windows(num_windows, stan_model):\n",
    "    prediction_errors = []\n",
    "    for i in range(num_windows):\n",
    "        data = get_train_and_test_for_sliding_window(datetime(2014, 1, 1) + timedelta(days=i * 7))\n",
    "        fit = stan_model.sampling(seed=COMMON_SEED, data=dict(\n",
    "            x_train=data['x_train'],\n",
    "            x_test=data['x_test'],\n",
    "            y=data['y_train'],\n",
    "            T=7,\n",
    "            N=len(data['x_train'])), iter=500)\n",
    "        check_convergence(fit)\n",
    "        y_pred_avg = fit.extract()['lambda'].mean()\n",
    "#         print(data)\n",
    "#         y_pred = historical_average(data['x_train'], data['y_train'])[data['x_test'] - 1].round()\n",
    "#         prediction_errors.append(y_pred - data['y_test'])\n",
    "#         print(y_pred)\n",
    "#         print(data['y_test'])\n",
    "#     return np.concatenate(prediction_errors)\n",
    "\n",
    "fit_stan_model_sliding_windows(1, model_hier_poisson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Performance Assessment\n",
    "RMSE, MAE, widht of 95% C.I.\n",
    "Cross-Validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Potential Improvements\n",
    "Discussion of problems and potential improvements:\n",
    "* Use contextual information, which is known to account for trends in the data.\n",
    "* Compare to some classic baseline models for time series prediction, e.g. Seasonal ARIMA with exogenous variables.\n",
    "* More sensitivity analysis for choice of prior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In particular, emphasize one main conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
